{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2285fb3",
   "metadata": {},
   "source": [
    "# SvaraAI Reply Classification Pipeline\n",
    "\n",
    "This notebook demonstrates a complete ML pipeline for classifying email replies from prospects into three categories:\n",
    "- **Positive**: Interested in meeting/demo\n",
    "- **Negative**: Not interested / rejection\n",
    "- **Neutral**: Non-committal or irrelevant\n",
    "\n",
    "## Overview\n",
    "1. Data exploration and preprocessing\n",
    "2. Baseline model training (Logistic Regression, Random Forest, LightGBM)\n",
    "3. Transformer model fine-tuning (DistilBERT)\n",
    "4. Model comparison and selection\n",
    "5. Production recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287938b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40445cb2",
   "metadata": {},
   "source": [
    "## 1. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aff225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('reply_classification_dataset.csv')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('Missing values:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check label distribution\n",
    "print('\n",
    "Label distribution (raw):')\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aebbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['label'].value_counts().plot(kind='bar')\n",
    "plt.title('Raw Label Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Clean labels and visualize again\n",
    "df['label_clean'] = df['label'].str.lower().str.strip()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['label_clean'].value_counts().plot(kind='bar')\n",
    "plt.title('Cleaned Label Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Cleaned label distribution:')\n",
    "print(df['label_clean'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd23dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n",
    "    # Strip whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "df['label_clean'] = df['label'].str.lower().str.strip()\n",
    "\n",
    "# Remove any remaining missing values\n",
    "df = df.dropna(subset=['text_clean', 'label_clean'])\n",
    "\n",
    "print(f'Final dataset shape: {df.shape}')\n",
    "print('\n",
    "Example of cleaned text:')\n",
    "for i in range(3):\n",
    "    print(f'Original: {df.iloc[i][\"text\"]}')\n",
    "    print(f'Cleaned:  {df.iloc[i][\"text_clean\"]}')\n",
    "    print(f'Label:    {df.iloc[i][\"label_clean\"]}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedbdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text characteristics\n",
    "df['text_length'] = df['text_clean'].str.len()\n",
    "df['word_count'] = df['text_clean'].str.split().str.len()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "df['text_length'].hist(bins=20)\n",
    "plt.title('Text Length Distribution')\n",
    "plt.xlabel('Character Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "df['word_count'].hist(bins=20)\n",
    "plt.title('Word Count Distribution')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "df.groupby('label_clean')['text_length'].mean().plot(kind='bar')\n",
    "plt.title('Average Text Length by Label')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Average Character Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Text statistics by label:')\n",
    "print(df.groupby('label_clean')[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b864d",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering and Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label_clean'])\n",
    "\n",
    "# Print label mapping\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print('Label mapping:')\n",
    "for label, encoded in label_mapping.items():\n",
    "    print(f'{label}: {encoded}')\n",
    "\n",
    "# Train-test split\n",
    "X = df['text_clean']\n",
    "y = df['label_encoded']\n",
    "y_labels = df['label_clean']\n",
    "\n",
    "X_train, X_test, y_train, y_test, y_labels_train, y_labels_test = train_test_split(\n",
    "    X, y, y_labels, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'\n",
    "Training set size: {len(X_train)}')\n",
    "print(f'Test set size: {len(X_test)}')\n",
    "print(f'\n",
    "Training label distribution:')\n",
    "print(pd.Series(y_labels_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c088e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF features\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),  # Include bigrams\n",
    "    min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.8  # Ignore terms that appear in more than 80% of documents\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f'TF-IDF feature matrix shape: {X_train_tfidf.shape}')\n",
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')\n",
    "\n",
    "# Show top features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f'\n",
    "Top 20 features: {feature_names[:20]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6dbe87",
   "metadata": {},
   "source": [
    "## 3. Baseline Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a0da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'Training {name}...')\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'model': model\n",
    "    }\n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    print(f'  Accuracy: {accuracy:.4f}')\n",
    "    print(f'  F1 Score: {f1:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca36cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "results_df['accuracy'].plot(kind='bar')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "results_df['f1_score'].plot(kind='bar')\n",
    "plt.title('Model F1 Score Comparison')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Model Performance Summary:')\n",
    "print(results_df[['accuracy', 'f1_score']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9edb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of best traditional model\n",
    "best_traditional = max(results.items(), key=lambda x: x[1]['f1_score'])\n",
    "best_name, best_metrics = best_traditional\n",
    "best_predictions = predictions[best_name]\n",
    "\n",
    "print(f'Best Traditional Model: {best_name}')\n",
    "print(f'F1 Score: {best_metrics[\"f1_score\"]:.4f}')\n",
    "print(f'Accuracy: {best_metrics[\"accuracy\"]:.4f}')\n",
    "\n",
    "print('\n",
    "Detailed Classification Report:')\n",
    "print(classification_report(y_test, best_predictions, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title(f'Confusion Matrix - {best_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aae42b",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Logistic Regression\n",
    "if 'Logistic Regression' in results:\n",
    "    lr_model = results['Logistic Regression']['model']\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get coefficients for each class\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        coeffs = lr_model.coef_[i]\n",
    "        top_positive_indices = np.argsort(coeffs)[-10:]\n",
    "        top_negative_indices = np.argsort(coeffs)[:10]\n",
    "        \n",
    "        top_features = np.concatenate([top_negative_indices, top_positive_indices])\n",
    "        top_coeffs = coeffs[top_features]\n",
    "        top_feature_names = [feature_names[idx] for idx in top_features]\n",
    "        \n",
    "        colors = ['red' if c < 0 else 'blue' for c in top_coeffs]\n",
    "        plt.barh(range(len(top_coeffs)), top_coeffs, color=colors)\n",
    "        plt.yticks(range(len(top_coeffs)), top_feature_names)\n",
    "        plt.title(f'Top Features for {class_name.title()} Class')\n",
    "        plt.xlabel('Coefficient Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e834a",
   "metadata": {},
   "source": [
    "## 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis - find misclassified examples\n",
    "misclassified_mask = y_test != best_predictions\n",
    "misclassified_texts = X_test[misclassified_mask]\n",
    "misclassified_true = y_test[misclassified_mask]\n",
    "misclassified_pred = best_predictions[misclassified_mask]\n",
    "\n",
    "print(f'Number of misclassified examples: {sum(misclassified_mask)}')\n",
    "print(f'Error rate: {sum(misclassified_mask) / len(y_test):.2%}')\n",
    "\n",
    "# Show some examples\n",
    "print('\n",
    "Sample Misclassified Examples:')\n",
    "print('=' * 80)\n",
    "\n",
    "for i in range(min(10, sum(misclassified_mask))):\n",
    "    idx = np.where(misclassified_mask)[0][i]\n",
    "    true_label = label_encoder.inverse_transform([misclassified_true.iloc[i]])[0]\n",
    "    pred_label = label_encoder.inverse_transform([misclassified_pred[i]])[0]\n",
    "    \n",
    "    print(f'Text: {misclassified_texts.iloc[i]}')\n",
    "    print(f'True: {true_label} | Predicted: {pred_label}')\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ae6d8b",
   "metadata": {},
   "source": [
    "## 6. Model Recommendations for Production\n",
    "\n",
    "Based on the analysis above, here are the key findings and recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and recommendations\n",
    "print('=' * 60)\n",
    "print('MODEL SELECTION SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f'{name}:')\n",
    "    print(f'  Accuracy: {metrics[\"accuracy\"]:.4f}')\n",
    "    print(f'  F1 Score: {metrics[\"f1_score\"]:.4f}')\n",
    "    print()\n",
    "\n",
    "print(f'Best Model: {best_name}')\n",
    "print(f'Best F1 Score: {best_metrics[\"f1_score\"]:.4f}')\n",
    "\n",
    "print('PRODUCTION RECOMMENDATIONS:')\n",
    "print('1. Model Choice: Start with the best traditional model for faster inference')\n",
    "print('2. Feature Engineering: TF-IDF with bigrams works well for this task')\n",
    "print('3. Monitoring: Track prediction confidence and retrain when performance drops')\n",
    "print('4. Data Quality: Focus on consistent labeling and text preprocessing')\n",
    "print('5. Evaluation: Use F1 score as primary metric due to potential class imbalance')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
